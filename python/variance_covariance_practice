#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Feb 18 11:46:48 2020

@author: adeela
"""

'''
    This file explores/implements variance, covaraince, pearson coorelation
    and which one to use in practice. PEARSON CORRELATION is preffered.
'''

import numpy as np 
import pandas as pd 

# =============================================================================
#       Few ways to create a MATRIX in numpy 
# =============================================================================

data = np.column_stack((np.arange(1,10), np.arange(1,10)))

#data = np.arange(1,21).reshape(10,2)
#data = pd.DataFrame({
#            'A': np.arange(1,10), 
#            'B': np.arange(1,10)
#        }).to_numpy()

data.shape
# =============================================================================
#   Variance/Standard Deviation  Calculation manually and via numpy
# =============================================================================
mean_col1 = np.mean(data[:, 0])
mean_col2 = np.mean(data[:, 1])

from functools import reduce

# see VECTORIZED implementation of the method
def calculate_varaince(X):
    #return reduce( lambda x,y: x+y , (np.square(mean_col1 - x) for x in X))/len(X)-1
    #return np.sum(np.square(X-np.mean(X)))/(len(X)-1)
    # THIS implementation is exactly as you would find the COVARIANCE between X and Y
    return np.sum(
            (X-np.mean(X))*
             (X-np.mean(X)))/(len(X)-1)

variance_col_1 = calculate_varaince(data[:, 0])
variance_col_2 = calculate_varaince(data[:, 1])

# ddof = specifies the N-ddof in the formula. if not specified then 
# the formula is divided by N  meaning we get population statistics

assert variance_col_1 ==  np.var(data[:, 0], ddof=1)

std_col_1 = np.sqrt(variance_col_1)
std_col_2 = np.sqrt(variance_col_2)


assert std_col_1 == np.std(data[:,0], ddof=1)

## STD INTERPRETATION
# each data value is std_value distance from the mean  
# Value of std closer to 'mean' means that data points are closer 


# =============================================================================
#                   Co-Varaince  
#    Measures how much each of the dimension varies from mean w.r.t. each other
#   COVARIANCE is used to measure the relationship between two dimensions to 
#   see if there is a relationship between two dimensions
#    e.g. number of hours studied vs grades on exam 
# =============================================================================

def calculate_covariance(X,Y):
    return np.sum(
            np.subtract(X, np.mean(X)) * 
            np.subtract(Y, np.mean(Y)))/(len(X)-1)


# <<IMPORTANT>> magnitude(the number itself) of covariance is not important.
# positive means two dimensions varies with eachother
# negative means two dimensions DOES NOT vary with eachother
# zero means two dimensions are independent

#<<IMPORTANT>>  covariance of the varible with itself is simply variance
    
covaraince_XY_p = calculate_covariance(data[:,0], data[:,1])
covaraince_XY_n = calculate_covariance(data[:,0], -1*data[:,1])
covaraince_XY_z = calculate_covariance(data[:,0], np.random.randn(9))

# numpy gives a covaraince matrix 
np.cov(data[:,0], data[:,1])

# =============================================================================
#       CoVaraiance Matrix
# =============================================================================

D = pd.DataFrame({
        'A': np.random.randn(10),
        'B': np.arange(1,11),
        'C': np.arange(1,11)*2.1
        }).to_numpy()

from itertools import product, combinations_with_replacement    

#cartesian product of indices
indices = product(range(0,D.shape[1]), range(0,D.shape[1]))

#for performance can use combinations w/o repetition because we are going to 
# get a symmetric matrix 
#indices_comb = combinations_with_replacement(range(0,len(D)), 2)

co_var = []

for i in indices:
    co_var.append(calculate_covariance(D[:,i[0]],D[:,i[1]]))

covariance_matrix = pd.DataFrame(np.array(co_var).reshape(3,3))


# =============================================================================
#           PEARSON Correlation Coefficient
#
# Its value is always in the range(-1, 1)  
# >> 1  means perfect positive correlation
# >> 0  means no relationship
# >> -1 means perfect negative correlation
#
# FORMULA : covaraince(X,Y) / std(X)*std(Y)
#
# =============================================================================

def calculate_pearson_correlation(X, Y):
    return round(calculate_covariance(X,Y)/
                 (np.std(X, ddof=1)* np.std(Y, ddof=1)), 1)    


X = D[:, 2]
Y = D[:, 1]
calculate_pearson_correlation(X,Y)
calculate_pearson_correlation(D[:, 0],Y)

#NUMPY way --> returns a correlation matrix 
np.corrcoef(X,Y)
np.corrcoef(D[:, 0],Y)

# Let's manually calculate the correlation matrix between ecah column of D

indices = product(range(0, D.shape[1]), range(0, D.shape[1]))

corr_list = []
for i in indices:
    corr_list.append(calculate_pearson_correlation(D[:,i[0]], D[:,i[1]]))

corr_matrix = pd.DataFrame(np.array(corr_list).reshape(D.shape[1], D.shape[1]))    


# =============================================================================
#       Covariance vs Correlation Coefficient
#   correlatiojn is preffered because its not affected by Scale and unit. Its  
#   values are between -1 and 1 
#   
#  <<IMPORTANT>> LOOK at the code example corr_matrix vs covariance_matrix
#               by looking at corr_matrix, we can see that 1 indicates 
#               that two columns are correlated, but in covaraince its show 
#               a number but hard to discern how much they are related
# =============================================================================

    
# =============================================================================
#   PLotting covaraince    
# =============================================================================


import matplotlib.pyplot as plt
import seaborn as sns
import yellowbrick as yb
from yellowbrick.features import Rank1D, Rank2D
#
#c = corr_matrix.corr()
#sns.heatmap(c)
#
#
#
#
#viz = Rank2D(['A','B','C'], algorithm='pearson')
#viz.fit(covariance_matrix)
#viz.transform(covariance_matrix)
##plt.plot(covariance_matrix)   
#
#sns.heatmap(corr_matrix)
##plt.plot(np.corrcoef(data))
#    
#plt.matshow(covariance_matrix)    
#plt.show()    
#    
#import matplotlib.pyplot as plt
#
#plt.matshow(dataframe.corr())
#plt.show()    
#    
    
    









    



























