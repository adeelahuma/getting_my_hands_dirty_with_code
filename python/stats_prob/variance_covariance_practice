#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Feb 18 11:46:48 2020

@author: adeela
"""

'''
    This file explores/implements variance, covaraince, pearson coorelation, 
    spearmean correlation coefficient, point biserail correlation etc
    and which one to use in practice. 
    PEARSON CORRELATION is preffered over covariance.
    
        
Which correlation to use based on variable Type:     
    Continous vs Continous                   -->  Paerson or Spearman
    Continous vs Categorical(2 categories)   -->  Point Biserial 
    Continous vs Categorical(>2 categories)  -->  ETA
    Categorical vs Categorical               -->  Chi Squared or Cramer's V
    
LINK --> 
    Correlation Analysis: https://statzilla.ru/application_en
    https://numpy.org/doc/1.18/reference/generated/numpy.corrcoef.html
    https://gradehub.com/blog/point-biserial/  
    https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.pearsonr.html
    https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.spearmanr.html
    https://www.statisticshowto.datasciencecentral.com/probability-and-statistics/correlation-coefficient-formula/
    Pearson vs Spearman --> https://stats.stackexchange.com/questions/11746/what-could-cause-big-differences-in-correlation-coefficient-between-pearsons-an?noredirect=1&lq=1
    Pearson or Spearman? --> https://stats.stackexchange.com/questions/8071/how-to-choose-between-pearson-and-spearman-correlation?noredirect=1&lq=1
    p-value in correclation -- >https://stats.stackexchange.com/questions/131037/what-is-the-deal-with-p-value-when-generating-pearsons-r-correlation-coeffi?answertab=oldest#tab-top
    https://datascience.stackexchange.com/questions/893/how-to-get-correlation-between-two-categorical-variable-and-a-categorical-variab
    
'''

import numpy as np 
import pandas as pd 

# =============================================================================
#       Few ways to create a MATRIX in numpy 
# =============================================================================

data = np.column_stack((np.arange(1,10), np.arange(1,10)))

#data = np.arange(1,21).reshape(10,2)
#data = pd.DataFrame({
#            'A': np.arange(1,10), 
#            'B': np.arange(1,10)
#        }).to_numpy()

data.shape
# =============================================================================
#   Variance/Standard Deviation  Calculation manually and via numpy
# =============================================================================
mean_col1 = np.mean(data[:, 0])
mean_col2 = np.mean(data[:, 1])

from functools import reduce

# see VECTORIZED implementation of the method
def calculate_varaince(X):
    #return reduce( lambda x,y: x+y , (np.square(mean_col1 - x) for x in X))/len(X)-1
    #return np.sum(np.square(X-np.mean(X)))/(len(X)-1)
    # THIS implementation is exactly as you would find the COVARIANCE between X and Y
    return np.sum(
            (X-np.mean(X))*
             (X-np.mean(X)))/(len(X)-1)

variance_col_1 = calculate_varaince(data[:, 0])
variance_col_2 = calculate_varaince(data[:, 1])

# ddof = specifies the N-ddof in the formula. if not specified then 
# the formula is divided by N  meaning we get population statistics

assert variance_col_1 ==  np.var(data[:, 0], ddof=1)

std_col_1 = np.sqrt(variance_col_1)
std_col_2 = np.sqrt(variance_col_2)


assert std_col_1 == np.std(data[:,0], ddof=1)

## STD INTERPRETATION
# each data value is std_value distance from the mean  
# Value of std closer to 'mean' means that data points are closer 


# =============================================================================
#                   Co-Varaince  
#    Measures how much each of the dimension varies from mean w.r.t. each other
#   COVARIANCE is used to measure the relationship between two dimensions to 
#   see if there is a relationship between two dimensions
#    e.g. number of hours studied vs grades on exam 
# =============================================================================

def calculate_covariance(X,Y):
    return np.sum(
            np.subtract(X, np.mean(X)) * 
            np.subtract(Y, np.mean(Y)))/(len(X)-1)


# <<IMPORTANT>> magnitude(the number itself) of covariance is not important.
# positive means two dimensions varies with eachother
# negative means two dimensions DOES NOT vary with eachother
# zero means two dimensions are independent

#<<IMPORTANT>>  covariance of the varible with itself is simply variance
    
covaraince_XY_p = calculate_covariance(data[:,0], data[:,1])
covaraince_XY_n = calculate_covariance(data[:,0], -1*data[:,1])
covaraince_XY_z = calculate_covariance(data[:,0], np.random.randn(9))

# numpy gives a covaraince matrix 
np.cov(data[:,0], data[:,1])

# =============================================================================
#       CoVaraiance Matrix
# =============================================================================

D = pd.DataFrame({
        'A': np.random.randn(10),
        'B': np.arange(1,11),
        'C': np.arange(1,11)*2.1
        }).to_numpy()

from itertools import product, combinations_with_replacement    

#cartesian product of indices
indices = product(range(0,D.shape[1]), range(0,D.shape[1]))

#for performance can use combinations w/o repetition because we are going to 
# get a symmetric matrix 
#indices_comb = combinations_with_replacement(range(0,len(D)), 2)

co_var = []

for i in indices:
    co_var.append(calculate_covariance(D[:,i[0]],D[:,i[1]]))

covariance_matrix = pd.DataFrame(np.array(co_var).reshape(3,3))


# =============================================================================
#           PEARSON Correlation Coefficient
# Assumes that each dataset is normally distributed 
# Its value is always in the range(-1, 1)  
# >> 1  means perfect positive correlation
# >> 0  means no relationship
# >> -1 means perfect negative correlation
#
# FORMULA : covaraince(X,Y) / std(X)*std(Y)
#
# =============================================================================

def calculate_pearson_correlation(X, Y):
    return round(calculate_covariance(X,Y)/
                 (np.std(X, ddof=1)* np.std(Y, ddof=1)), 1)    


X = D[:, 2]
Y = D[:, 1]
calculate_pearson_correlation(X,Y)
calculate_pearson_correlation(D[:, 0],Y)

#NUMPY way --> returns a correlation matrix 
np.corrcoef(X,Y)
np.corrcoef(D[:, 0],Y)

# Let's manually calculate the correlation matrix between ecah column of D

indices = product(range(0, D.shape[1]), range(0, D.shape[1]))

corr_list = []
for i in indices:
    corr_list.append(calculate_pearson_correlation(D[:,i[0]], D[:,i[1]]))

corr_matrix = pd.DataFrame(np.array(corr_list).reshape(D.shape[1], D.shape[1]))    


# =============================================================================
#       Covariance vs Correlation Coefficient
#   correlatiojn is preffered because its not affected by Scale and unit. Its  
#   values are between -1 and 1 
#   
#  <<IMPORTANT>> LOOK at the code example corr_matrix vs covariance_matrix
#               by looking at corr_matrix, we can see that 1 indicates 
#               that two columns are correlated, but in covaraince its show 
#               a number but hard to discern how much they are related
# =============================================================================

    
# =============================================================================
#   PLotting covaraince    
# =============================================================================


import matplotlib.pyplot as plt
import seaborn as sns
import yellowbrick as yb
from yellowbrick.features import Rank1D, Rank2D
#
#c = corr_matrix.corr()
#sns.heatmap(c)
#
#
#
#
#viz = Rank2D(['A','B','C'], algorithm='pearson')
#viz.fit(covariance_matrix)
#viz.transform(covariance_matrix)
##plt.plot(covariance_matrix)   
#
#sns.heatmap(corr_matrix)
##plt.plot(np.corrcoef(data))
#    
#plt.matshow(covariance_matrix)    
#plt.show()    
#    
#import matplotlib.pyplot as plt
#
#plt.matshow(dataframe.corr())
#plt.show()    
#    
    
    
# =============================================================================
#           Point Biserial Correlation
#   The point biserial correlation is used to measure the relationship 
#   between a binary variable, x, and a continuous variable, y. 
#
# =============================================================================

df = pd.DataFrame({
r        'col_1': np.random.randint(50,100, size=50), 
        'col_2': np.random.choice([0,1], 50),
        'col_3': np.random.choice(['A','B'], 50),
        'col_4': np.random.choice(['K', 'L', 'M'], 50)
        })
df = df.astype({'col_1': 'int', 'col_2': 'category', 'col_3': 'category', 
                'col_4': 'category'})    

df['col_5'] = df['col_1'] * 10
df['col_6'] = df['col_1'] * -10
df['col_7'] = np.random.randint(0,100, 50)

df.head()

# Pandas Dataframe Method:
# computer pairwise correlation CONYINOUS columns ONLY. Default method is pearson
df.corr()
df.corr(method='spearman') # Question: why spearman coefficient's values are low? 

# Numpy Method: it takes two data arrays 
np.corrcoef(df['col_1'], df['col_2'])    # continous vs binary
np.corrcoef(df['col_2'], df['col_2'])    # binary vs binary
np.corrcoef(df['col_1'], df['col_3'])    # continous vs continous

# Point Biserial 
from scipy import stats
stats.pointbiserialr(df['col_1'], df['col_2'])

# H0 --> correlation is zero
# H1 --> corelation is not zero
# Now look at p-value to accept or reject null hypothesis


# =============================================================================
#           Pearson VS Spearman
#Why the big difference
#
#>> If your data is normally distributed or uniformly distributed, 
#I would think that Spearman's and Pearson's correlation should be fairly similar.
#>> If they are giving very different results as in your case (.65 versus .30), 
# my guess is that you have skewed data or outliers, and that outliers are 
# leading Pearson's correlation to be larger than Spearman's correlation. 
# I.e., very high values on X might co-occur with very high values on Y.
#>>Your first step should be to look at the scatter plot.
#>> In general, such a big difference between Pearson and Spearman is a red f
# lag suggesting that the Pearson correlation may not be a useful summary of 
# the association between your two variables, or you should transform one or 
# both variables before using Pearson's correlation, or you should remove or 
# adjust outliers before using Pearson's correlation.

# <<Refer to link above for more details>>

#<<WHICH ONE TO USE>>
# compute both of the coefficients and look at the differences. 
# In many cases, they will be exactly the same, so you don't need to worry.

#If however, they are different then you need to look at whether or not you met
# the assumptions of Pearsons (constant variance and linearity) and if these are
# not met, you are probably better off using Spearmans.

# =============================================================================



# =============================================================================
#           Cramer's V (phi)
# =============================================================================


# =============================================================================
#           CHi Squared ( ki Squared) 
# - LINK: 
#   https://www.mathsisfun.com/data/chi-square-test.html
#   Python Practical example : https://machinelearningmastery.com/chi-squared-test-for-machine-learning/
#   ML Usage: https://www.quora.com/How-is-chi-test-used-for-feature-selection-in-machine-learning
# - Calculated between categorical variables
# - Chi square test tells you that which features are better related with the 
#   outcome variable. Smaller p value means more closely related with the outcome.
# - The variables are considered independent if the observed and expected 
#   frequencies are similar, that the levels of the variables do not interact, 
#   are not dependent
# - H0: the observed frequencies for a categorical variable match the
#           expected frequencies for the categorical variable./ 
#       Independent varaibles
#   H1: Dependent variables
#   Interpretaion: if p-value <significae level(0.05) Reject Null 
#                  (Meaning varaibles are Dependent) 
#
# =============================================================================

from scipy.stats import chi2_contingency

## BOTH COLUMNS has two disntinct categories
observed_freq= pd.crosstab(df['col_2'], df['col_3'] ).to_numpy()
chi, pvalue, ddof, expected = chi2_contingency(observed_freq)

if pvalue < 0.05:  # NULL must go 
    print('Dependent variables')
else: 
    print('Independent variables')    

## COL_4 has 3 categories
ob_freq = pd.crosstab(df['col_2'], df['col_4'])
chi2_contingency(ob_freq)


# =============================================================================
#           Theil's U        
# =============================================================================


# =============================================================================
#           Correlation Ratio/ETA Correlation
# =============================================================================


# =============================================================================
#       Contigency Table
# - The table was called a contingency table, by Karl Pearson, because the 
#   intent is to help determine whether one variable is contingent upon or 
#   depends upon the other variable. 
#   For example, does an interest in math or science depend on gender, 
#       or are they independent?

# =============================================================================
pd.crosstab(df['col_2'], df['col_3'] )

























